{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.tensor(np.pi)\n",
    "a2 = torch.tensor(math.pi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4102) tensor(0.8570)\n"
     ]
    }
   ],
   "source": [
    "p = 3\n",
    "\n",
    "\n",
    "b1 = torch.log(torch.tensor(2*np.pi**(p/2)))\n",
    "b2 = torch.log(torch.tensor(3/4*np.pi))\n",
    "print(b1,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WatsonDistribution(nn.Module):\n",
    "    \"\"\"\n",
    "    Multivariate Watson distribution class\n",
    "    \"\"\"\n",
    "    def __init__(self,p, para_init=None):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.mu = nn.Parameter(torch.ones(3))\n",
    "        self.kappa = nn.Parameter(torch.rand(self.p,self.p))\n",
    "        \n",
    "\n",
    "W = WatsonDistribution(p=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.state_dict()['mu'] *= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('mu', tensor([3., 3., 3.])),\n",
       "             ('kappa',\n",
       "              tensor([[0.2341, 0.1961, 0.5224],\n",
       "                      [0.1311, 0.9199, 0.1865],\n",
       "                      [0.9313, 0.2443, 0.9681]]))])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([39, 39, 39])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([3,3,3])\n",
    "A = torch.tensor([[1,1,1],[2,2,2],[10,10,10]])\n",
    "torch.matmul(x,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Parameter.__repr__ of Parameter containing:\n",
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], requires_grad=True)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = nn.Parameter(torch.ones(3,3))\n",
    "mu.__repr__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 10\n",
    "p=3\n",
    "mu = torch.zeros(p,K)\n",
    "for j in range(K):\n",
    "        val = 1 if j % 2 == 0 else -1\n",
    "        mu[(j*int(p/K)),j] = val\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(K)*1/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor([0.4365, 0.6558, 0.0900, 0.2037, 0.5743])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "v = torch.rand(5)\n",
    "print(torch.linalg.norm(v/torch.linalg.norm(v,dim=0)))\n",
    "print(v/torch.linalg.norm(v,dim=0))\n",
    "print(torch.linalg.norm(nn.functional.normalize(v,dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           LogBackward0        49.66%     291.000us        53.58%     314.000us     314.000us             1  \n",
      "                                              aten::log         8.02%      47.000us         8.02%      47.000us      47.000us             1  \n",
      "                                    aten::empty_strided         5.12%      30.000us         5.12%      30.000us      30.000us             1  \n",
      "                                         aten::softplus         4.95%      29.000us         4.95%      29.000us      29.000us             1  \n",
      "                                aten::softplus_backward         4.27%      25.000us         4.27%      25.000us      25.000us             1  \n",
      "                                                 detach         4.10%      24.000us         4.10%      24.000us      12.000us             2  \n",
      "                                              aten::div         3.92%      23.000us         3.92%      23.000us      23.000us             1  \n",
      "                                         aten::uniform_         3.07%      18.000us         3.07%      18.000us      18.000us             1  \n",
      "autograd::engine::evaluate_function: SoftplusBackwar...         2.39%      14.000us         8.36%      49.000us      49.000us             1  \n",
      "      autograd::engine::evaluate_function: LogBackward0         2.39%      14.000us        55.97%     328.000us     328.000us             1  \n",
      "                                             aten::rand         2.22%      13.000us         7.17%      42.000us      42.000us             1  \n",
      "                                            aten::empty         1.88%      11.000us         1.88%      11.000us      11.000us             1  \n",
      "                                      SoftplusBackward0         1.71%      10.000us         5.97%      35.000us      35.000us             1  \n",
      "                                           aten::detach         1.54%       9.000us         5.63%      33.000us      16.500us             2  \n",
      "                                       aten::empty_like         1.54%       9.000us         6.66%      39.000us      39.000us             1  \n",
      "                                        aten::ones_like         1.37%       8.000us         8.36%      49.000us      49.000us             1  \n",
      "                        torch::autograd::AccumulateGrad         1.02%       6.000us         3.07%      18.000us      18.000us             1  \n",
      "autograd::engine::evaluate_function: torch::autograd...         0.51%       3.000us         3.58%      21.000us      21.000us             1  \n",
      "                                            aten::fill_         0.34%       2.000us         0.34%       2.000us       2.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 586.000us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softplus = nn.Softplus()\n",
    "\n",
    "\n",
    "with torch.autograd.profiler.profile() as prof:\n",
    "\n",
    "    y = softplus(torch.log(nn.Parameter(torch.rand(1))))\n",
    "    y.backward()\n",
    "    \n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogWatson(nn.Module):\n",
    "    \"\"\"\n",
    "    Logarithmic Multivariate Watson distribution class\n",
    "    \"\"\"\n",
    "    def __init__(self,p, para_init=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.p = p\n",
    "        self.mu = nn.Parameter(torch.ones(self.p))\n",
    "        self.kappa = nn.Parameter(torch.tensor([1.]))\n",
    "\n",
    "        if para_init is not None:\n",
    "            print('Custom Initilization of')\n",
    "            #for special initialization of patamters\n",
    "\n",
    "            \n",
    "    def log_kummer(self, a, c, kappa):\n",
    "        # inspiration form Morten Bessel function\n",
    "        # Gamma based? see Mardia A.18\n",
    "        logKum = torch.log(torch.tensor(0.5))\n",
    "        return logKum\n",
    "\n",
    "    def log_norm_constant(self):\n",
    "        logC =  torch.lgamma(torch.tensor([self.p/2])) \\\n",
    "               - torch.log(torch.tensor(2 * np.pi**(self.p/2))) \\\n",
    "               - self.log_kummer(0.5, self.p/2, self.kappa)\n",
    "\n",
    "        return logC\n",
    "\n",
    "    def log_pdf(self,x):\n",
    "        # Why Transpose in the end? see LL_Torch_Watson\n",
    "        logpdf = self.log_norm_constant() + self.kappa * (self.mu @ x)**2\n",
    "        return logpdf\n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.log_pdf(X)\n",
    "    \n",
    "m = [LogWatsonMultivariate(p=3)]*10\n",
    "\n",
    "# Watson = nn.ModuleList([w(p=3) for w in m])\n",
    "# for Wat in Watson:\n",
    "#     print(Wat.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139674543406864\n",
      "139674543402544\n",
      "139674543399856\n",
      "139674543391408\n",
      "139674543405088\n"
     ]
    }
   ],
   "source": [
    "m = [LogWatson(p=3) for _ in range(5)]\n",
    "\n",
    "for w in m:\n",
    "    print(id(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x3 and 2x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [154], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m W1 \u001b[38;5;241m=\u001b[39m LogWatsonMultivariate(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mW1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [153], line 32\u001b[0m, in \u001b[0;36mLogWatsonMultivariate.log_pdf\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_pdf\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Why Transpose in the end? see LL_Torch_Watson\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     logpdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_norm_constant() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkappa \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logpdf\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x3 and 2x3)"
     ]
    }
   ],
   "source": [
    "W1 = LogWatsonMultivariate(p=3)\n",
    "\n",
    "W1.log_pdf(torch.tensor([[1.,2.,3.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 12., 15., 21.])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1.,1.,1.])\n",
    "b = torch.Tensor([[2.,2.,2.],[4.,4.,4.],[5.,5.,5.],[7.,7.,7.]])\n",
    "# [6, 12, 15, 21]\n",
    "\n",
    "torch.matmul(a,b.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogSumExp test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3370)\n",
      "tensor(5.3370)\n"
     ]
    }
   ],
   "source": [
    "def lsum(x):\n",
    "    return x.max() + torch.log(torch.exp(x-x.max()).sum())\n",
    "\n",
    "def LS(x):\n",
    "    return torch.logsumexp(x, dim=0)\n",
    "\n",
    "xv = torch.rand(137)**2\n",
    "print(lsum(xv))\n",
    "print(LS(xv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
